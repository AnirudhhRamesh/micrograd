{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micrograd - Arnie's version\n",
    "\n",
    "I built this project as a way to solidify my understanding of the Stanford/DeepLearningAI Coursera course.\n",
    "\n",
    "Before taking the class (June start - July end), I had watched Andrej Karpathy's Micrograd lecture (around May), but not since then.\n",
    "\n",
    "To really test my understanding, I built this out *without* re-watching Karpathy's lecture. That is, I built this with a rough understanding of autograd, and thought through first principles of differentiation, partial derivatives, chain_rule, software design, back_propagation, linear regression, optimizers, loss functions.\n",
    "\n",
    "Note:\n",
    "- I did have a working micrograd implementation, however my loss was incorrect.\n",
    "- After manually calculating the backprop and still not figuring out, I skimmed through Karpathy's micrograd code.\n",
    "- I took the opportunity to:\n",
    "-> Add in additional methods I hand't thought like __sub__, __rsub__, ....\n",
    "-> Clean-up my implementation of Neuron (notably, passing in input_feature and a single x sample, rather than the x as array itself)\n",
    "\n",
    "In the end, I realized the problem was with that I forgot to call the zero_grad() which led to the loss bouncing since the grads were compounding across iterations\n",
    "\n",
    "In principle, I don't take shortcuts, but given my urgency to progress as fast as possible in ML and since I already had a working implementation (with correct intuition), it did not make sense to spend more time debugging (I can always do that later).\n",
    "\n",
    "Extra notes: Obviously I did not use Copilot xD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val(value=18.0000, grad=0.0000, parents=(6.0 * 3.0))\n",
      "Val(value=2.0000, grad=0.0000, parents=(6.0 * 0.3333333333333333))\n"
     ]
    }
   ],
   "source": [
    "from micrograd.engine.value import Value\n",
    "\n",
    "a = Value(6.0)\n",
    "b = Value(3.0)\n",
    "\n",
    "c = a * b\n",
    "print(c)\n",
    "\n",
    "d = a / b\n",
    "print(d)\n",
    "\n",
    "e = Value(100.0)\n",
    "f = e.log()\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f.log()\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0529\n",
      "Epoch [200/1000], Loss: 0.0434\n",
      "Epoch [300/1000], Loss: 0.0426\n",
      "Epoch [400/1000], Loss: 0.0425\n",
      "Epoch [500/1000], Loss: 0.0425\n",
      "Epoch [600/1000], Loss: 0.0425\n",
      "Epoch [700/1000], Loss: 0.0425\n",
      "Epoch [800/1000], Loss: 0.0425\n",
      "Epoch [900/1000], Loss: 0.0425\n",
      "Epoch [1000/1000], Loss: 0.0425\n",
      "Prediction for x=6.0 is y_pred=11.816327056431131 (Expected is 12)\n"
     ]
    }
   ],
   "source": [
    "from micrograd.nets.SimpleLinearRegression import SimpleLinearRegression\n",
    "from micrograd.loss_functions.MSE import MSE\n",
    "from micrograd.optimizers.SimpleOptimizer import SimpleOptimizer\n",
    "\n",
    "from micrograd.engine.value import Value\n",
    "\n",
    "# Set-up model, optimizers and loss function\n",
    "model = SimpleLinearRegression()\n",
    "criterion = MSE()\n",
    "\n",
    "num_epochs = 1000\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = SimpleOptimizer(model.parameters(), lr)\n",
    "\n",
    "# Dataset\n",
    "x = [Value(1.0), Value(4.0), Value(9.0)]\n",
    "y = [Value(1.0), Value(8.0), Value(18.0)]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Need to applying the logistic activation!\n",
    "    y_pred = [model([xi]) for xi in x]\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.value:.4f}')\n",
    "\n",
    "test_x = [Value(6.0)]\n",
    "test_y_pred = model(test_x)\n",
    "\n",
    "print(f\"Prediction for x={test_x[0].value} is y_pred={test_y_pred.value} (Expected is 12)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot Dog Classifier\n",
    "Inspired by Jian-Yang from Silicon Valley, I built out a simple hot dog classifer.\n",
    "\n",
    "I downloaded a dataset from HuggingFace and used my Micrograd implementation to train and evaluate test images :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (10.4.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 200\n",
      "Validation samples: 50\n"
     ]
    }
   ],
   "source": [
    "# Load data and pre-process it\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from micrograd.engine.value import Value\n",
    "\n",
    "IMAGE_SIZE = 225\n",
    "\n",
    "def load_and_preprocess_images(folder_path, label, image_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.resize((image_size, image_size))\n",
    "                img = img.convert('L')  # Convert to grayscale\n",
    "                img_array = np.array(img).flatten() / 255.0  # Normalize to [0, 1]\n",
    "                images.append([Value(float(pixel)) for pixel in img_array])\n",
    "                labels.append(Value(float(label)))\n",
    "    return images, labels\n",
    "\n",
    "# Load training data\n",
    "train_hotdog_path = 'dataset/hotdog_nothotdog/train/hotdog'\n",
    "train_nothotdog_path = 'dataset/hotdog_nothotdog/train/not_hotdog'\n",
    "\n",
    "x_train_hotdog, y_train_hotdog = load_and_preprocess_images(train_hotdog_path, 1, IMAGE_SIZE)\n",
    "x_train_nothotdog, y_train_nothotdog = load_and_preprocess_images(train_nothotdog_path, 0, IMAGE_SIZE)\n",
    "\n",
    "x_train = x_train_hotdog + x_train_nothotdog\n",
    "y_train = y_train_hotdog + y_train_nothotdog\n",
    "\n",
    "# Load validation data\n",
    "val_hotdog_path = 'dataset/hotdog_nothotdog/val/hotdog'\n",
    "val_nothotdog_path = 'dataset/hotdog_nothotdog/val/not_hotdog'\n",
    "\n",
    "x_valid_hotdog, y_valid_hotdog = load_and_preprocess_images(val_hotdog_path, 1, IMAGE_SIZE)\n",
    "x_valid_nothotdog, y_valid_nothotdog = load_and_preprocess_images(val_nothotdog_path, 0, IMAGE_SIZE)\n",
    "\n",
    "x_valid = x_valid_hotdog + x_valid_nothotdog\n",
    "y_valid = y_valid_hotdog + y_valid_nothotdog\n",
    "\n",
    "# Shuffle the training data\n",
    "combined = list(zip(x_train, y_train))\n",
    "np.random.shuffle(combined)\n",
    "x_train, y_train = zip(*combined)\n",
    "\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 16.74500741199737 - (step: 0.02)\n",
      "Epoch [2/1000], Loss: 15.274396967145977 - (step: 0.02)\n",
      "Epoch [3/1000], Loss: 0.913862302068128 - (step: 0.00040946396473271184)\n",
      "Epoch [4/1000], Loss: 1.8847307975486143 - (step: 0.00043039983075195647)\n",
      "Epoch [5/1000], Loss: 1.2451070174722156 - (step: 0.0002118502356626603)\n",
      "Epoch [6/1000], Loss: 1.2386506537410762 - (step: 0.0016896293335791539)\n",
      "Epoch [7/1000], Loss: 1.2331419018233445 - (step: 0.0019838251696586945)\n",
      "Epoch [8/1000], Loss: 1.281563892455867 - (step: 0.0006519202676529401)\n",
      "Epoch [9/1000], Loss: 1.176251056197677 - (step: 0.00012568049427623163)\n",
      "Epoch [10/1000], Loss: 1.2122863239164492 - (step: 0.001604228288120476)\n",
      "Epoch [11/1000], Loss: 1.4143988579222595 - (step: 0.000774691484114436)\n",
      "Epoch [12/1000], Loss: 0.9827225319650521 - (step: 0.0008212440072663189)\n",
      "Epoch [13/1000], Loss: 2.4318005171876194 - (step: 0.001)\n",
      "Epoch [14/1000], Loss: 0.7396792858725348 - (step: 2.8068175173366256e-06)\n",
      "Epoch [15/1000], Loss: 0.746330758528612 - (step: 7.624877275230711e-06)\n",
      "Epoch [16/1000], Loss: 0.7659822790874106 - (step: 8.9548368254643e-06)\n",
      "Epoch [17/1000], Loss: 0.7897824797804147 - (step: 6.245357220262861e-07)\n",
      "Epoch [18/1000], Loss: 0.7913337756507768 - (step: 6.340597259148525e-06)\n",
      "Epoch [19/1000], Loss: 0.8076018604243365 - (step: 1.6842467490027948e-06)\n",
      "Epoch [20/1000], Loss: 0.8117265616469453 - (step: 8.450965149830493e-06)\n",
      "Epoch [21/1000], Loss: 0.8329170803785962 - (step: 3.5034944237906237e-06)\n",
      "Epoch [22/1000], Loss: 0.841149194370172 - (step: 6.3522620929732886e-06)\n",
      "Epoch [23/1000], Loss: 0.8559172420712042 - (step: 0.00012700183973949613)\n",
      "Epoch [24/1000], Loss: 1.2096475965659188 - (step: 0.0016987458232900524)\n",
      "Epoch [25/1000], Loss: 1.4705685585152315 - (step: 0.0011664294900957176)\n",
      "Epoch [26/1000], Loss: 0.7649447598061206 - (step: 7.768369487237028e-06)\n",
      "Epoch [27/1000], Loss: 0.7854383105123234 - (step: 9.299841736462741e-06)\n",
      "Epoch [28/1000], Loss: 0.8099790738880058 - (step: 1.5715071346784492e-06)\n",
      "Epoch [29/1000], Loss: 0.8138285222134088 - (step: 4.376974394371045e-06)\n",
      "Epoch [30/1000], Loss: 0.8246197464851492 - (step: 4.412392246056209e-06)\n",
      "Epoch [31/1000], Loss: 0.8352718476220795 - (step: 4.887904359027194e-06)\n",
      "Epoch [32/1000], Loss: 0.8468136100200364 - (step: 8.705874706207894e-06)\n",
      "Epoch [33/1000], Loss: 0.8670178611183046 - (step: 0.00024181688866387605)\n",
      "Epoch [34/1000], Loss: 1.5790766112468921 - (step: 0.0017117845363670916)\n",
      "Epoch [35/1000], Loss: 0.8606551102285417 - (step: 0.0011537987249207193)\n",
      "Epoch [36/1000], Loss: 15.760396624794588 - (step: 0.02)\n",
      "Epoch [37/1000], Loss: 2.053001288231 - (step: 0.001)\n"
     ]
    }
   ],
   "source": [
    "from micrograd.nets.ComplexLogisticRegression import ComplexLogisticRegression\n",
    "\n",
    "from micrograd.loss_functions.BinaryCrossEntropy import BinaryCrossEntropy\n",
    "from micrograd.optimizers.SimpleOptimizer import SimpleOptimizer\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "model = ComplexLogisticRegression(image_size=IMAGE_SIZE)\n",
    "criterion = BinaryCrossEntropy()\n",
    "\n",
    "num_epochs = 1000\n",
    "lr = 0.02\n",
    "\n",
    "optimizer = SimpleOptimizer(model.parameters(), lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    y_pred = [model(xi) for xi in x_train]\n",
    "\n",
    "    # for prediction in y_pred:\n",
    "        # print(f\"Prediction: {prediction}\")\n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Dynamically adjust the learning rate\n",
    "    if (loss.value < 20.0):\n",
    "        optimizer.set_learn_rate(0.02)    \n",
    "\n",
    "    if (loss.value < 10.0):\n",
    "        optimizer.set_learn_rate(0.001)\n",
    "\n",
    "    if (loss.value < 2.0):\n",
    "        optimizer.set_learn_rate(random.uniform(0.0001, 0.002))\n",
    "\n",
    "    if (loss.value < 0.85):\n",
    "        optimizer.set_learn_rate(random.uniform(0.0000001, 0.00001))\n",
    "\n",
    "    # Early stopping\n",
    "    if (loss.value < 0.70):\n",
    "        break\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.value} - (step: {optimizer.get_learn_rate()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val(value=0.7196, grad=1.0000, parents=(143.914047016878 * 0.005))\n",
      "0.71957023508439\n",
      "Loss: 0.7196\n",
      "Epoch [], Loss: 0.7196\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(loss.value)\n",
    "print(f'Loss: {loss.value:.4f}')\n",
    "print(f'Epoch [], Loss: {loss.value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 with confidence 0.61 for actual: 1.0\n",
      "Prediction: 0 with confidence 0.40 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.54 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.65 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 1.0\n",
      "Prediction: 0 with confidence 0.47 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.62 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.55 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.52 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.52 for actual: 1.0\n",
      "Prediction: 0 with confidence 0.49 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 1.0\n",
      "Prediction: 0 with confidence 0.49 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.60 for actual: 1.0\n",
      "Prediction: 0 with confidence 0.49 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.52 for actual: 1.0\n",
      "Prediction: 0 with confidence 0.45 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.53 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.56 for actual: 1.0\n",
      "Prediction: 0 with confidence 0.50 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.50 for actual: 1.0\n",
      "Prediction: 1 with confidence 0.54 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.68 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.69 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.58 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.62 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.57 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.72 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.56 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.54 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.57 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.56 for actual: 0.0\n",
      "Prediction: 0 with confidence 0.50 for actual: 0.0\n",
      "Prediction: 0 with confidence 0.49 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.64 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.63 for actual: 0.0\n",
      "Prediction: 0 with confidence 0.48 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.64 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 0.0\n",
      "Prediction: 0 with confidence 0.48 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.66 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.51 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.52 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.52 for actual: 0.0\n",
      "Prediction: 1 with confidence 0.70 for actual: 0.0\n",
      "Correct guesses: 22, Wrong guesses: 28, Total: 50 - Accuracy: 44.0%\n",
      "Loss: Val(value=0.7196, grad=1.0000, parents=(143.914047016878 * 0.005))\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "n = len(x_valid)\n",
    "correct_guesses = 0\n",
    "wrong_guesses = 0\n",
    "\n",
    "y_pred = [model(xi) for xi in x_valid]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for pred in y_pred:\n",
    "    if pred.value <= 0.5:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "    \n",
    "for i in range(n):\n",
    "    print(f'Prediction: {predictions[i]} with confidence {y_pred[i].value:.2f} for actual: {y_valid[i].value}')\n",
    "\n",
    "    if predictions[i] == y_valid[i].value:\n",
    "        correct_guesses += 1\n",
    "    else:\n",
    "        wrong_guesses += 1\n",
    "\n",
    "print(f\"Correct guesses: {correct_guesses}, Wrong guesses: {wrong_guesses}, Total: {n} - Accuracy: {(correct_guesses/n)*100}%\")\n",
    "print(f\"Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val(value=4.6052, grad=0.0000, parents=(2.3025850929940455 + 2.3025850929940455))\n"
     ]
    }
   ],
   "source": [
    "y_pred = Value(0.1)\n",
    "y = Value(1.0)\n",
    "\n",
    "loss_test = Value(0.0)\n",
    "loss_test += (-(y * y_pred.log()) - ((Value(1.0)-y)*(Value(1.0)-y_pred).log()))\n",
    "loss_test += (-(y * y_pred.log()) - ((Value(1.0)-y)*(Value(1.0)-y_pred).log()))\n",
    "\n",
    "\n",
    "print(loss_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
