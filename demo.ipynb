{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micrograd - Arnie's version\n",
    "\n",
    "I built this project as a way to solidify my understanding of the Stanford/DeepLearningAI Coursera course.\n",
    "\n",
    "Before taking the class (June start - July end), I had watched Andrej Karpathy's Micrograd lecture (around May), but not since then.\n",
    "\n",
    "To really test my understanding, I built this out *without* re-watching Karpathy's lecture. That is, I built this with a rough understanding of autograd, and thought through first principles of differentiation, partial derivatives, chain_rule, software design, back_propagation, linear regression, optimizers, loss functions.\n",
    "\n",
    "Note:\n",
    "- I did have a working micrograd implementation, however my loss was incorrect.\n",
    "- After manually calculating the backprop and still not figuring out, I skimmed through Karpathy's micrograd code.\n",
    "- I took the opportunity to:\n",
    "-> Add in additional methods I hand't thought like __sub__, __rsub__, ....\n",
    "-> Clean-up my implementation of Neuron (notably, passing in input_feature and a single x sample, rather than the x as array itself)\n",
    "\n",
    "In the end, I realized the problem was with that I forgot to call the zero_grad() which led to the loss bouncing since the grads were compounding across iterations\n",
    "\n",
    "In principle, I don't take shortcuts, but given my urgency to progress as fast as possible in ML and since I already had a working implementation (with correct intuition), it did not make sense to spend more time debugging (I can always do that later).\n",
    "\n",
    "Extra notes: Obviously I did not use Copilot xD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in ./.venv/lib/python3.12/site-packages (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.venv/lib/python3.12/site-packages (from pydantic) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.12/site-packages (from pydantic) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "\n",
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val(value=18.0000, grad=0.0000, parents=(6.0 * 3.0))\n",
      "Val(value=2.0000, grad=0.0000, parents=(6.0 * 0.3333333333333333))\n"
     ]
    }
   ],
   "source": [
    "from micrograd.engine.value import Value\n",
    "\n",
    "a = Value(6.0)\n",
    "b = Value(3.0)\n",
    "\n",
    "c = a * b\n",
    "print(c)\n",
    "\n",
    "d = a / b\n",
    "print(d)\n",
    "\n",
    "e = Value(100.0)\n",
    "f = e.log()\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f.log()\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0575\n",
      "Epoch [200/1000], Loss: 0.0442\n",
      "Epoch [300/1000], Loss: 0.0427\n",
      "Epoch [400/1000], Loss: 0.0425\n",
      "Epoch [500/1000], Loss: 0.0425\n",
      "Epoch [600/1000], Loss: 0.0425\n",
      "Epoch [700/1000], Loss: 0.0425\n",
      "Epoch [800/1000], Loss: 0.0425\n",
      "Epoch [900/1000], Loss: 0.0425\n",
      "Epoch [1000/1000], Loss: 0.0425\n",
      "Prediction for x=6.0 is y_pred=11.816328060532037 (Expected is 12)\n"
     ]
    }
   ],
   "source": [
    "from micrograd.nets.SimpleLinearRegression import SimpleLinearRegression\n",
    "from micrograd.loss_functions.MSE import MSE\n",
    "from micrograd.optimizers.SimpleOptimizer import SimpleOptimizer\n",
    "\n",
    "from micrograd.engine.value import Value\n",
    "\n",
    "# Set-up model, optimizers and loss function\n",
    "model = SimpleLinearRegression()\n",
    "criterion = MSE()\n",
    "\n",
    "num_epochs = 1000\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = SimpleOptimizer(model.parameters(), lr)\n",
    "\n",
    "# Dataset\n",
    "x = [Value(1.0), Value(4.0), Value(9.0)]\n",
    "y = [Value(1.0), Value(8.0), Value(18.0)]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Need to applying the logistic activation!\n",
    "    y_pred = [model([xi]) for xi in x]\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.value:.4f}')\n",
    "\n",
    "test_x = [Value(6.0)]\n",
    "test_y_pred = model(test_x)\n",
    "\n",
    "print(f\"Prediction for x={test_x[0].value} is y_pred={test_y_pred.value} (Expected is 12)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot Dog Classifier\n",
    "Inspired by Jian-Yang from Silicon Valley, I built out a simple hot dog classifer.\n",
    "\n",
    "I downloaded a dataset from HuggingFace and used my Micrograd implementation to train and evaluate test images :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (10.4.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 200\n",
      "Validation samples: 50\n"
     ]
    }
   ],
   "source": [
    "# Load data and pre-process it\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from micrograd.engine.value import Value\n",
    "\n",
    "IMAGE_SIZE = 225\n",
    "\n",
    "def load_and_preprocess_images(folder_path, label, image_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.resize((image_size, image_size))\n",
    "                img = img.convert('L')  # Convert to grayscale\n",
    "                img_array = np.array(img).flatten() / 255.0  # Normalize to [0, 1]\n",
    "                images.append([Value(float(pixel)) for pixel in img_array])\n",
    "                labels.append(Value(float(label)))\n",
    "    return images, labels\n",
    "\n",
    "# Load training data\n",
    "train_hotdog_path = 'dataset/hotdog_nothotdog/train/hotdog'\n",
    "train_nothotdog_path = 'dataset/hotdog_nothotdog/train/not_hotdog'\n",
    "\n",
    "x_train_hotdog, y_train_hotdog = load_and_preprocess_images(train_hotdog_path, 1, IMAGE_SIZE)\n",
    "x_train_nothotdog, y_train_nothotdog = load_and_preprocess_images(train_nothotdog_path, 0, IMAGE_SIZE)\n",
    "\n",
    "x_train = x_train_hotdog + x_train_nothotdog\n",
    "y_train = y_train_hotdog + y_train_nothotdog\n",
    "\n",
    "# Load validation data\n",
    "val_hotdog_path = 'dataset/hotdog_nothotdog/val/hotdog'\n",
    "val_nothotdog_path = 'dataset/hotdog_nothotdog/val/not_hotdog'\n",
    "\n",
    "x_valid_hotdog, y_valid_hotdog = load_and_preprocess_images(val_hotdog_path, 1, IMAGE_SIZE)\n",
    "x_valid_nothotdog, y_valid_nothotdog = load_and_preprocess_images(val_nothotdog_path, 0, IMAGE_SIZE)\n",
    "\n",
    "x_valid = x_valid_hotdog + x_valid_nothotdog\n",
    "y_valid = y_valid_hotdog + y_valid_nothotdog\n",
    "\n",
    "# Shuffle the training data\n",
    "combined = list(zip(x_train, y_train))\n",
    "np.random.shuffle(combined)\n",
    "x_train, y_train = zip(*combined)\n",
    "\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Validation samples: {len(x_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 18.39591028953272 - (step: 0.04)\n",
      "Epoch [2/1000], Loss: 18.132157515113217 - (step: 0.04)\n",
      "Epoch [3/1000], Loss: 17.985969556895956 - (step: 0.04)\n",
      "Epoch [4/1000], Loss: 17.79856883488213 - (step: 0.04)\n",
      "Epoch [5/1000], Loss: 17.57129163371318 - (step: 0.04)\n",
      "Epoch [6/1000], Loss: 14.607587045929161 - (step: 0.02)\n",
      "Early stopping at epoch 7, loss: 2.4208\n"
     ]
    }
   ],
   "source": [
    "from micrograd.nets.ComplexLogisticRegression import ComplexLogisticRegression\n",
    "\n",
    "from micrograd.loss_functions.BinaryCrossEntropy import BinaryCrossEntropy\n",
    "from micrograd.optimizers.SimpleOptimizer import SimpleOptimizer\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "model = ComplexLogisticRegression(image_size=IMAGE_SIZE)\n",
    "criterion = BinaryCrossEntropy()\n",
    "\n",
    "num_epochs = 1000\n",
    "lr = 0.02\n",
    "\n",
    "optimizer = SimpleOptimizer(model.parameters(), lr)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    y_pred = [model(xi) for xi in x_train]\n",
    "\n",
    "    # for prediction in y_pred:\n",
    "        # print(f\"Prediction: {prediction}\")\n",
    "\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Dynamically adjust the learning rate\n",
    "    if (loss.value < 20.0):\n",
    "        optimizer.set_learn_rate(0.04)\n",
    "    \n",
    "    if (loss.value < 15.0):\n",
    "        optimizer.set_learn_rate(0.02)    \n",
    "\n",
    "    if (loss.value < 10.0):\n",
    "        optimizer.set_learn_rate(0.001)\n",
    "\n",
    "    if (loss.value < 2.0):\n",
    "        optimizer.set_learn_rate(random.uniform(0.0001, 0.002))\n",
    "\n",
    "    if (loss.value < 0.85):\n",
    "        optimizer.set_learn_rate(random.uniform(0.0000001, 0.00001))\n",
    "\n",
    "    # Early stopping\n",
    "    if (loss.value < 0.70):\n",
    "        print(f'Early stopping at epoch {epoch+1}, loss: {loss.value:.4f}')\n",
    "        break\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.value} - (step: {optimizer.get_learn_rate()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model...\n",
      "{\"layers\":[{\"input_features\":225,\"output_features\":3,\"activation\":\"linear_activation\",\"neurons\":[{\"values\":[0.8326694434135926,0.8653004940246046,0.3575845181427726,0.1947515208968238,0.029102446590822294,0.8109089889425899,0.04853141427575596,0.6245817133439922,0.08544539703794027,0.30272382227925576,0.26675499157529653,0.8834605003728313,0.26869382868199526,0.48020011629024434,0.3552782953539262,0.25445039077070797,0.09730523567084494,0.06427740051879416,0.398665332186434,0.061530901886086974,0.19993514416263755,0.007028087036286916,0.09016701665663628,0.20587455358745801,0.47581884377844763,0.7742250603489343,0.16015109789667054,0.3524284788489767,0.6782049302904016,0.4731117653159907,0.04494060589865303,0.9716073609031944,0.741290362638135,0.5855292208911226,0.39708157076095807,0.06204290999238257,0.5616020145439548,0.44935801357467425,0.5284439367146707,0.1693121025933208,0.041208162534073375,0.9213932798748579,0.05647854384199753,0.6998856836004788,0.9793317593006574,0.4527323697689929,0.4497584922044834,0.5422708841393049,0.1586523993815754,0.14716954760251533,0.1928407459994706,0.8386858503663239,0.19202892512088648,0.4671963838665689,0.10448634080372853,0.5425283397334366,0.9133854584615799,0.4912796250561391,0.3923408500624693,0.656468592257182,0.4564814336817535,0.8277324956945961,0.026681387523107128,0.5769421147211359,0.9140927728107571,0.8340938047036579,0.2944457704112522,0.17529692281509182,0.08258058274253854,0.6928890459937036,0.23981751403330068,0.22944558492402864,0.03259243166751408,0.4983766048904863,0.36710565242280346,0.15770054436331452,0.4008150907796296,0.2244140051408479,0.06901279491713408,0.9700038744570094,0.47728147018715394,0.8340146463797623,0.46119767129963785,0.972781949150394,0.8477571514680318,0.6001556242299569,0.17062830730419468,0.8857314128332333,0.1620534852228413,0.239024499958347,0.1760712880896463,0.8339867712099804,0.6746578100210543,0.9750876708482953,0.18723055881005748,0.5934400328851818,0.17753465129518903,0.761611262205334,0.5124457686332291,0.7893063375877234,0.2315451128950955,0.7509357879243593,0.11751628330240124,0.9647828750244253,0.3869307512509155,0.6024677365519451,0.8659111859748317,0.7887009309015983,0.2612073906900429,0.4855538436178663,0.26189122539598053,0.9217782115808881,0.46810896986201045,0.21233365336061882,0.562641396411465,0.9437091363367939,0.6197711371655614,0.7022402176675242,0.935820433403764,0.7084508606379805,0.7898031961779282,0.9487081319691063,0.10706259449721221,0.35646277484667294,0.7261324481066226,0.007411905332448919,0.4990564337612764,0.5114240458467804,0.8701414496141892,0.30234371857331777,0.0498806094281078,0.1333900492303505,0.6615193539431417,0.3151314358725636,0.020602445534677966,0.7610362671239467,0.35795703502325493,0.9620209462146434,0.7529433802998787,0.7954394341660105,0.23880324853815504,0.19777499276678392,0.3787025425208033,0.601493739062198,0.919869457657863,0.9255720936603927,0.5208319240140304,0.5218801605891472,0.04721166188347195,0.4025969851891054,0.3216570434902493,0.4950014442877456,0.3042372066845187,0.33960985611529654,0.24807434059900582,0.5199640659499174,0.5142021889977793,0.6697981675947653,0.9134511802337327,0.8373233571397768,0.016300082389651405,0.6963504644518255,0.7728229508252081,0.18569439732510487,0.49063366217745036,0.11749221146573513,0.5958357435443272,0.9786192759341463,0.965948862404696,0.4196890990605187,0.7844493387856751,0.09772892628460664,0.05827376477627055,0.5864939011433498,0.4927697053532702,0.8833718938699974,0.5882275684915034,0.8845516856366423,0.549300269614966,0.21540764310199031,0.7994176049613655,0.1085356003966636,0.32758639765148817,0.39574806516415933,0.056102021843630584,0.8990723575761107,0.32857963505816284,0.22370137483382907,0.37737197338594364,0.6628232195018111,0.11021438775831954,0.7506653166051332,0.4035485180356892,0.5360620883850173,0.565500363663798,0.9117670447590185,0.025973674142270507,0.806317341060325,0.29018616089161614,0.1708584679654295,0.08308240012275676,0.7470454889540532,0.8725324000013961,0.40307026449123073,0.17997727164323432,0.11274698643465267,0.20839200060311766,0.8726671675749691,0.12027823075296547,0.3182013419967327,0.3021884810005086,0.3224371043728028,0.5331502058152984,0.5695235309283345,0.3585617493285715,0.7573411550582633,0.7869453434851397,0.9218111621921297,0.23647655234601958,0.7027501643815034,0.36304821181924324,0.9807709745765889,0.8884279792565264,0.9191836828608816,0.9034350791535779,-0.016017954152030982]},{\"values\":[0.8902819103530689,0.6075480433959719,0.0692079559221714,0.6441460703923301,0.4306505370631741,0.5469318628849329,0.20739356822426347,0.23959162546313212,0.0575058695831904,0.667632955315765,0.22979841303411996,0.5063141950065873,0.8839222139846072,0.9464094345653714,0.8270848399110198,0.2969604370402139,0.5097048304389166,0.6568984220490183,0.03312507133471501,0.5010395966654466,0.984944768410998,0.9917374092962801,0.1550885998953346,0.6203888312061515,0.9161709226032109,0.7120828857687653,0.23268632230917957,0.9687550571124489,0.1430304867176766,0.672688519478275,0.017499830100472263,0.5326874524610696,0.46469065887448224,0.8873995453718176,0.5364991719316563,0.10779550937332798,0.07016970846452195,0.5140953662755559,0.8242572988959908,0.041385871930922004,0.8769501447320883,0.39642034607164073,0.007228041910772872,0.7066684191827347,0.2538993269403715,0.6751628100750697,0.24203203064421736,0.0835927593490843,0.6369070563655401,0.9107971503932183,0.7547000452788015,0.14601013332374538,0.9213700774420421,0.16171858821938134,0.936408504612556,0.49765740120067187,0.7960329964230141,0.11286690070326667,0.6445788435354897,0.023476941610395693,0.8243648648423464,0.3907146726993839,0.6137079336104417,0.4483443620632963,0.4385150246562928,0.30492093699959444,0.8017293564379848,0.7415853134772953,0.07025213169431038,0.23563941767056631,0.1729664451122448,0.48134251488675883,0.9023254336165698,0.4829890478873526,0.9563095642507097,0.8727342343238149,0.34023013833194465,0.8543317469025927,0.5160940046603566,0.5424346046830006,0.13038266786902672,0.5113371531350462,0.10587304267361765,0.06305959060492812,0.07682884103975865,0.5275851672614003,0.2977093077856814,0.5665222101114997,0.8454699040988616,0.5977300013133506,0.055161744023222534,0.7356227964452173,0.8152732229015407,0.7165356794495538,0.9618358724771886,0.2773949575877178,0.4834090401467436,0.6704583784331409,0.10692908529593252,0.02627542244307158,0.18413552445456563,0.28726133555696265,0.6441548018948675,0.3248258086962394,0.9129972378165753,0.5520495006765965,0.4623218408651078,0.8313611712884575,0.14017684890528734,0.2707090241077606,0.07317674398689754,0.0046763947886803645,0.41550092592823124,0.5894972917473905,0.8435138132266469,0.9698833088778477,0.8133853675242931,0.5670720164942255,0.1700670532793267,0.9527730788410589,0.8595260097536053,0.37386972477171576,0.6693136211102102,0.18392682573249616,0.12886040290125392,0.327815573684309,0.9404394954469658,0.99870315405085,0.4444675849305366,0.4514744577184854,0.649774379025847,0.7393891050889247,0.332733019089156,0.9058495206184881,0.9079385888532233,0.45330604635946553,0.2055897404679607,0.37080067693310337,0.7430149839890007,0.8627226509942911,0.9617485177022381,0.653210534606525,0.179748074013959,0.174320111631304,0.12162718833107411,0.11109966578146314,0.3349925017923232,0.11379590685052014,0.1507745196535542,0.430340207637011,0.9324877523844648,0.9406214708426954,0.7358482636026968,0.013346742656506408,0.8663560945853649,0.8233880694280914,0.7069461916371124,0.10826117865181577,0.9636781018853036,0.5998000505572966,0.04142152157095795,0.6617529460375903,0.9288669371538923,0.5555911204222386,0.11870203052001048,0.5901705840092499,0.4274569595432764,0.12277015013989136,0.8931648410569228,0.24156673910729706,0.03327081475085522,0.48991327331631607,0.6798350892120586,0.8576135603422942,0.29264367974114586,0.8173825390325433,0.5982352917636512,0.9677754451578756,0.5304938612315255,0.6332915726915916,0.6811068787718038,0.8350913264571335,0.34043265827190416,0.22203437301326326,0.3435623714993562,0.8369398144327134,0.41716461539065486,0.5045715091762828,0.6919776961649936,0.8786968297692476,0.18793201371686735,0.9416930280533296,0.6574994423110885,0.2794173692845677,0.1620443893119432,0.29727168068545745,0.492565263979341,0.15279934873721812,0.6129421683649264,0.5784443308360828,0.5654596396402044,0.2281807358381015,0.4852565659598508,0.3636711644179363,0.6405715671805612,0.7610646979922143,0.33671275967046127,0.2828954029707703,0.6293429338279493,0.8704711669078974,0.48820665844545114,0.060987903545316254,0.1555696216207351,0.5737492646954705,0.5872181303962772,0.09059723287541323,0.7082677350341843,0.7096946260497671,0.287980633558188,0.6641735493632419,0.048034362410765495,0.5928857859947071,0.9847582257173173,0.21266383893834478,0.1643624843014995,-0.006602870296288859]},{\"values\":[0.846528877310151,0.8403559103987991,0.910890859968746,0.7239769424922943,0.1492012983433649,0.32235230229103434,0.1983140288061306,0.6177635120690668,0.5758826425087367,0.7364508967849415,0.492650964900162,0.3512291473800932,0.9109440250055675,0.828693329598329,0.04693993500530965,0.5579262911631093,0.5452219622133164,0.7844477391003478,0.10916296917565906,0.054754692119886615,0.19622824104199388,0.7175401887352383,0.23182153570455902,0.2929314062154746,0.7233318130283425,0.03778600215762484,0.09340443740811234,0.17554694585181405,0.18546371749292864,0.22099254171279228,0.24086601920223139,0.27925287342516925,0.8014942670532004,0.4280219350232019,0.9409350770308657,0.198327448866076,0.4299623239823625,0.9236207701355271,0.17300479898810514,0.03822879577984964,0.15717513229401583,0.08035130085688387,0.002992572134950476,0.3237818528708969,0.505010559420253,0.9615083936795784,0.7059587113885302,0.7577248787625113,0.8998046001710692,0.7831700612637158,0.4089045619505615,0.5715585636679096,0.3490930384035663,0.9375167294045906,0.04647090091534657,0.6405369537967495,0.5527555229992315,0.9662538319781891,0.9846011717530235,0.08468642417992017,0.5727321200036786,0.17642501962806095,0.6808884222984177,0.7815701372805633,0.7940889690276451,0.8177944213657834,0.8621636773224638,0.628065736545621,0.13784605914092454,0.32946932238274174,0.6831900449707302,0.4390975188660158,0.653545251182741,0.481169303009065,0.27022421128241597,0.11672654333261681,0.9391515351772568,0.10113604706119277,0.310346899276802,0.6794615956773276,0.5335080775441309,0.6879907474479511,0.45312546270473575,0.2952546521689679,0.4309744987222862,0.7802954757999886,0.5574085044658198,0.497554210432607,0.9128814424488056,0.10363520199032816,0.048430917934540046,0.18228149992112067,0.2591983644234348,0.5550321556802449,0.23986652582678994,0.8156501856816993,0.12499946258185672,0.926284380164376,0.12213676755339986,0.1886681864681603,0.21418332788258496,0.33670764773015005,0.5660749631682829,0.9595568775972173,0.4458902037937188,0.0000331436805723305,0.7312215551656072,0.7997981714681793,0.03688263307239144,0.006467148046393123,0.8758246425977977,0.34681842416047765,0.7867622448903636,0.7347492921766953,0.05974257066601079,0.4280886662939019,0.5593627710574237,0.17623881051852433,0.8201548402512774,0.010096109683189067,0.18764368622884828,0.25288761477580474,0.9689729852593667,0.7787850886638595,0.9384327514793386,0.07429585835019979,0.28826189907320493,0.7826397523794221,0.42581778532849557,0.3759066767641138,0.6934768082958755,0.030032531030104104,0.5717942166624209,0.8074427423791972,0.5493394096457801,0.12399178455038101,0.12442662964560629,0.37139384548836124,0.6538903712978968,0.45761799861387054,0.9718565352097946,0.1419892415191788,0.9659326413995749,0.5633636394354217,0.16914263174963243,0.20350233239534665,0.9409669358201996,0.6419917946023795,0.1500696210961467,0.8011827648421882,0.873306756418494,0.21523691678105603,0.4634611134380494,0.751039114684304,0.5555481847473966,0.7153018802079811,0.49963466345790697,0.28282959743086206,0.8457128231651786,0.5346651181795574,0.981177887626027,0.6548970366293086,0.9952691205353448,0.9061792390239228,0.3544440105266378,0.9583449206136015,0.4221066140939967,0.6731779486737782,0.1727218906615796,0.7823290931647108,0.30318680932898223,0.920937844088277,0.4949734311756302,0.5824968420442093,0.30926708140417614,0.02761425350624639,0.27044683127216274,0.6664697116711434,0.3729370461766034,0.09656064968152779,0.6666246958119042,0.13671190521174956,0.44809029347919616,0.19982803420330508,0.41666782302966093,0.3777893132125302,0.15415326177497163,0.22066023770225637,0.08450013645167022,0.499642835716964,0.6879334346208424,0.3603608244962609,0.7313313491782186,0.754224502728972,0.12498089587883747,0.98765841650116,0.6820219104676909,0.8572642445433492,0.9420296000739982,0.4961264367449902,0.9831535019301032,0.9931850325240855,0.37156944825523314,0.793595040338921,0.13626971451615952,0.11953328842611657,0.24203363962021485,0.8908409960506064,0.9480550769530558,0.3307036608164472,0.5004143932766513,0.4099461424472988,0.8298522431216023,0.6453983921578957,0.5412499731032793,0.7882054891473743,0.8123486163649948,0.5548152744412331,0.07060093345578243,0.9318720680333075,0.6155769263959013,0.7481757418992208,0.4059451791276902,0.5839213682204694,0.23625299651138404,-0.022147367482984195]}]},{\"input_features\":3,\"output_features\":1,\"activation\":\"sigmoid_activation\",\"neurons\":[{\"values\":[0.02330427882740005,-0.29240151909399204,0.19148091032573153,-0.03225174125723444]}]}]}\n"
     ]
    }
   ],
   "source": [
    "# Write the model to a file\n",
    "import json\n",
    "\n",
    "exported_model = model.export()\n",
    "model_json = exported_model.model_dump_json()\n",
    "print(model_json)\n",
    "\n",
    "# Write the model to a file\n",
    "with open('model_weights.json', 'w') as f:\n",
    "    f.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing model...\n",
      "Imported model!\n",
      "Model successfully imported and validated.\n"
     ]
    }
   ],
   "source": [
    "from micrograd.nets.NeuralNet import NeuralNet\n",
    "from micrograd.schemas.schemas import ModelSchema\n",
    "\n",
    "# Read the model from the file\n",
    "with open('model_weights.json', 'r') as f:\n",
    "    model_json = f.read()\n",
    "\n",
    "# Parse and validate the JSON using pydantic\n",
    "model_schema = ModelSchema.model_validate_json(model_json)\n",
    "\n",
    "# Create a new NeuralNet instance from the validated schema\n",
    "imported_model = NeuralNet.from_json(model_schema)\n",
    "\n",
    "print(\"Model successfully imported and validated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val(value=2.4208, grad=1.0000, parents=(484.155516258618 * 0.005))\n",
      "2.42077758129309\n",
      "Loss: 2.4208\n",
      "Epoch [], Loss: 2.4208\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(loss.value)\n",
    "print(f'Loss: {loss.value:.4f}')\n",
    "print(f'Epoch [], Loss: {loss.value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct guesses: 25, Wrong guesses: 25, Total: 50 - Accuracy: 50.0%\n",
      "Loss: Val(value=2.4208, grad=1.0000, parents=(484.155516258618 * 0.005))\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "n = len(x_valid)\n",
    "correct_guesses = 0\n",
    "wrong_guesses = 0\n",
    "\n",
    "y_pred = [model(xi) for xi in x_valid]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for pred in y_pred:\n",
    "    if pred.value <= 0.5:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "    \n",
    "for i in range(n):\n",
    "    # print(f'Prediction: {predictions[i]} with confidence {y_pred[i].value:.2f} for actual: {y_valid[i].value}')\n",
    "\n",
    "    if predictions[i] == y_valid[i].value:\n",
    "        correct_guesses += 1\n",
    "    else:\n",
    "        wrong_guesses += 1\n",
    "\n",
    "print(f\"Correct guesses: {correct_guesses}, Wrong guesses: {wrong_guesses}, Total: {n} - Accuracy: {(correct_guesses/n)*100}%\")\n",
    "print(f\"Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct guesses: 25, Wrong guesses: 25, Total: 50 - Accuracy: 50.0%\n",
      "Loss: Val(value=2.4208, grad=1.0000, parents=(484.155516258618 * 0.005))\n"
     ]
    }
   ],
   "source": [
    "# Testing the imported model\n",
    "# Test the model\n",
    "n = len(x_valid)\n",
    "correct_guesses = 0\n",
    "wrong_guesses = 0\n",
    "\n",
    "y_pred = [imported_model(xi) for xi in x_valid]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for pred in y_pred:\n",
    "    if pred.value <= 0.5:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "    \n",
    "for i in range(n):\n",
    "    # print(f'Prediction: {predictions[i]} with confidence {y_pred[i].value:.2f} for actual: {y_valid[i].value}')\n",
    "\n",
    "    if predictions[i] == y_valid[i].value:\n",
    "        correct_guesses += 1\n",
    "    else:\n",
    "        wrong_guesses += 1\n",
    "\n",
    "print(f\"Correct guesses: {correct_guesses}, Wrong guesses: {wrong_guesses}, Total: {n} - Accuracy: {(correct_guesses/n)*100}%\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val(value=4.6052, grad=0.0000, parents=(2.3025850929940455 + 2.3025850929940455))\n"
     ]
    }
   ],
   "source": [
    "y_pred = Value(0.1)\n",
    "y = Value(1.0)\n",
    "\n",
    "loss_test = Value(0.0)\n",
    "loss_test += (-(y * y_pred.log()) - ((Value(1.0)-y)*(Value(1.0)-y_pred).log()))\n",
    "loss_test += (-(y * y_pred.log()) - ((Value(1.0)-y)*(Value(1.0)-y_pred).log()))\n",
    "\n",
    "\n",
    "print(loss_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
